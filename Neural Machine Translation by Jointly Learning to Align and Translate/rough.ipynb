{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_CONFIG = {'dictonary_size': 10, 'features': 64, 'hidden_size': 128, 'max_sentence_length': 10}\n",
    "dict_size = TEMP_CONFIG['dictonary_size']\n",
    "features = TEMP_CONFIG['features']\n",
    "hidden_size = TEMP_CONFIG['hidden_size']\n",
    "max_sentence_length = TEMP_CONFIG['max_sentence_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(10, 5)\n",
    "rnn = nn.RNN(5, 20, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 6, 5])\n",
      "torch.Size([1, 6, 20]) torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.randint(low=0, high=5, size=(1, 6))\n",
    "print(rand.shape)\n",
    "embs = emb(rand)\n",
    "print(embs.shape)\n",
    "outs, hidden = rnn(embs, torch.zeros(1, 1, 20))\n",
    "print(outs.shape, hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, max_sentence_length, dictionary_size, features, hidden_size) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dictionary_size = dictionary_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.features = features\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.emb = nn.Embedding(dictionary_size, features)\n",
    "        self.rnn = nn.RNN(input_size=features, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, batch, hidden_state):\n",
    "        embeddings = self.emb(batch)\n",
    "        print(embeddings.shape)\n",
    "        output, hidden_state = self.rnn(embeddings, hidden_state)\n",
    "        return output, hidden_state\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, max_sentence_length, dictionary_size, hidden_size, output_size) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.dictionary_size = dictionary_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.dictionary_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_sentence_length)\n",
    "        # self.attn_combined = nn.Linear(self.hidden_size*3, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size*3, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, batch, hidden_state, encoder_outputs):\n",
    "        embeddings = self.emb(batch)\n",
    "        cat = torch.cat([embeddings[0], hidden_state[0]], dim=1)\n",
    "        attn = self.attn(cat)\n",
    "        attn_weights = F.softmax(attn, dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs)\n",
    "        output = torch.cat([embeddings[0], attn_applied[0], hidden_state[0]], dim=1).unsqueeze(0)\n",
    "        output, hidden_state = self.gru(output, hidden_state)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(max_sentence_length, dict_size, features, hidden_size)\n",
    "rand_int = torch.randint(low=0, high=dict_size, size=(1, max_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 0, 9, 7, 5, 1, 5, 0, 9, 8]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "hidden_0 = encoder.init_hidden()\n",
    "output, hidden = encoder(rand_int, hidden_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 128]), torch.Size([1, 1, 128]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(max_sentence_length=max_sentence_length, dictionary_size=20, hidden_size=hidden_size, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int = torch.randint(low=0, high=dict_size, size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_0 = decoder.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 4)\n",
    "mat2 = torch.randn(1, 4, 5)\n",
    "res = torch.bmm(input, mat2)\n",
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.2863, -2.3585, -2.3466, -2.2032, -2.2372, -2.3966, -2.1776, -2.2995,\n",
       "          -2.3824, -2.3653]], grad_fn=<LogSoftmaxBackward0>),\n",
       " tensor([[[ 0.2300, -0.4224,  0.3249, -0.2062, -0.4335,  0.8271,  0.0484,\n",
       "           -0.3034, -0.7401, -0.3866,  0.1695, -0.1123, -0.2303,  0.2145,\n",
       "            0.3402,  0.0562, -0.2733,  0.2462, -0.5417, -0.1870, -0.1122,\n",
       "           -0.0526, -0.1126,  0.4738, -0.2139, -0.1675,  0.3899, -0.2158,\n",
       "            0.1488, -0.3018, -0.2774,  0.0140, -0.4432, -0.0302,  0.2154,\n",
       "           -0.3638, -0.2621,  0.1858, -0.4662,  0.3248, -0.1305,  0.1152,\n",
       "            0.2403, -0.1253,  0.4422, -0.2238,  0.3515, -0.1218, -0.1680,\n",
       "           -0.1718, -0.0535, -0.3999, -0.1258,  0.0321, -0.0918,  0.1172,\n",
       "            0.2668,  0.1775, -0.4352, -0.1927,  0.5146,  0.2775,  0.1411,\n",
       "            0.2379, -0.3669,  0.6606,  0.0108, -0.4858,  0.0624, -0.3505,\n",
       "           -0.0713, -0.4220,  0.2365, -0.4199,  0.2361, -0.0405,  0.4378,\n",
       "           -0.5760,  0.3931,  0.5880, -0.4566,  0.0984,  0.0448, -0.4123,\n",
       "            0.5046, -0.2182,  0.0152,  0.5720, -0.0062,  0.3013,  0.2970,\n",
       "           -0.6355,  0.5960,  0.7500,  0.3562, -0.3841,  0.0550,  0.4042,\n",
       "           -0.0579,  0.3146, -0.1515,  0.1397, -0.1596,  0.4618,  0.3985,\n",
       "           -0.2746, -0.4202, -0.0144, -0.1691,  0.2076, -0.3150, -0.2303,\n",
       "            0.0342,  0.1824, -0.3717,  0.5087, -0.4082,  0.0092, -0.7560,\n",
       "            0.4408, -0.1813, -0.1854, -0.1583,  0.1792,  0.0328,  0.3947,\n",
       "            0.3819,  0.6039]]], grad_fn=<StackBackward0>),\n",
       " tensor([[0.0827, 0.0866, 0.0562, 0.1553, 0.1179, 0.1321, 0.1004, 0.0845, 0.0825,\n",
       "          0.1019]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(rand_int, hidden_0, output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
