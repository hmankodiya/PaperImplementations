dataset_config:
  desc: coco-2014 caption dataset
  # train:
  #   dataset_path: /home/harsh/Desktop/Projects/PaperImplementations/ShowAndTell/coco-2014/dataset_json/train_dataset.json
  #   return_dict: true
  #   padding: longest
  #   image_size:
  #     - 224
  #     - 224
  #   max_length: 40
  #   sampling_fn:
  #     sampling_fn_name: random_sample
  #     # sampling_fn_name: choose_index
  #     # index: 0

  val:
    dataset_path: /home/harsh/Desktop/Projects/PaperImplementations/ShowAndTell/coco-2014/dataset_json/val_dataset.json
    return_dict: true
    padding: longest
    image_size:
      - 224
      - 224
    max_length: 40
    sampling_fn:
      sampling_fn_name: random_sample

tokenizer_config:
  tokenizer_name: gpt2
  tokenizer_path: "openai-community/gpt2"

model_config:
  showandtell_model:
    model_name: showandtell_gpt2
    model_path: /media/harsh/OS/Linux/Projects/ShowAndTell/clip_gpt2_fulltraining_init/checkpoint-4500/pytorch_model.bin # showandtell model weights
    generation_config:
      temperature: 0.5
      num_beams: 10
      do_sample: true
      
  image_model:
    model_name: clipvision
    # model_path: "openai/clip-vit-base-patch32"
    freeze: true
    # config:
    #   hidden_size: 768
    #   image_size: 518
    #   patch_size: 14

  text_model:
    model_name: gpt2
    # model_path: openai-community/gpt2 # text encoder weights
    config:
      add_cross_attention: true

trainer_config:
  run_name: "eval_beamsearch"
  output_dir: "/home/harsh/Desktop/Projects/PaperImplementations/ShowAndTell/showandtell_gpt2/TrainingLogs/eval"
  overwrite_output_dir: true
  report_to: "tensorboard"
  logging_steps: 1
  logging_strategy: steps
  per_device_eval_batch_size: 45
  predict_with_generate: true

  # # train args
  # eval_on_start: false
  # do_train: true
  # do_eval: true
  # eval_strategy: epoch
  # num_train_epochs: 300
  # per_device_train_batch_size: 50
  # prediction_loss_only: false
  # # log configs
  # # save_steps:
  # save_strategy: steps
  # save_steps: 300
  # save_total_limit: 3
  # remove_unused_columns: false
  # save_safetensors: false
  # # train configs
  # max_grad_norm: 0.3
  # seed: random
  # warmup_ratio: 0.05
  # learning_rate: 1.0E-4
  # lr_scheduler_type: cosine
  # optim: "adamw_torch"
  # eval_accumulation_steps: 1
