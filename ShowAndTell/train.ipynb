{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import Dinov2Config\n",
    "\n",
    "from dataset import ImageCaptionDataset, ImageTextCollator\n",
    "from model import (\n",
    "    load_tokenizer,\n",
    "    load_dinov2_image_encoder,\n",
    "    load_lstm_text_encoder,\n",
    "    load_show_and_tell,\n",
    "    load_lightning_model,\n",
    ")\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_image_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_text_model_config,\n",
    "    get_model_config,\n",
    "    get_trainer_config,\n",
    "    get_show_and_tell_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset_config', 'tokenizer_config', 'model_config', 'trainer_config'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = read_yaml(\"./configs/train_config.yaml\")\n",
    "_, (train_split_config, val_split_config, _) = get_split_config(config)\n",
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./coco-2014/dataset.json',\n",
       " {'return_dict': True,\n",
       "  'padding': 'longest',\n",
       "  'image_size': [518, 518],\n",
       "  'max_length': 512},\n",
       " 'choose_index',\n",
       " {'index': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_path, train_sampling_config, train_dataset_config = get_dataset_config(train_split_config)\n",
    "if train_sampling_config:\n",
    "    sampling_fn_name, sampling_fn_args = (\n",
    "        train_sampling_config.pop(\"sampling_fn_name\", None),\n",
    "        train_sampling_config,\n",
    "    )\n",
    "train_dataset_path, train_dataset_config, sampling_fn_name, sampling_fn_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_path, val_sampling_config, val_dataset_config = get_dataset_config(\n",
    "    val_split_config\n",
    ")\n",
    "\n",
    "if val_sampling_config:\n",
    "    val_sampling_fn_name, val_sampling_fn_args = (\n",
    "        val_sampling_config.pop(\"sampling_fn_name\", None),\n",
    "        val_sampling_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2', 'openai-community/gpt2', {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer_name, tokenizer_path, tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257,\n",
       " {'bos_token': '<|startoftext|>',\n",
       "  'eos_token': '<|endoftext|>',\n",
       "  'unk_token': '<|endoftext|>',\n",
       "  'pad_token': '<|pad|>'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")\n",
    "# tokenizer.add_bos_token('<|startoftext|>')\n",
    "tokenizer.vocab_size, tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sampling function 'None' should be either a callable type or str, found type <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageCaptionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_fn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_fn_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_dataset_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/PaperImplementations/ShowAndTell/dataset.py:141\u001b[0m, in \u001b[0;36mImageCaptionDataset.__init__\u001b[0;34m(self, tokenizer, dataset_path, return_tensors, return_dict, sampling_fn, sampling_fn_args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_fn_args \u001b[38;5;241m=\u001b[39m sampling_fn_args \u001b[38;5;28;01mif\u001b[39;00m sampling_fn_args \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampling function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampling_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be either a callable type or str, found type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sampling_fn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, SIZE)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "\u001b[0;31mValueError\u001b[0m: Sampling function 'None' should be either a callable type or str, found type <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "train_dataset = ImageCaptionDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_path=train_dataset_path,\n",
    "    sampling_fn=sampling_fn_name,\n",
    "    sampling_fn_args=sampling_fn_args,\n",
    "    return_tensors=None,\n",
    "    **train_dataset_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 518, 518), 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "batch[\"pixel_values\"].shape, len(batch[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model dry run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'showandtell_model': {'model_name': 'showandtell', 'model_path': None},\n",
       " 'image_model': {'model_name': 'dinov2',\n",
       "  'model_path': './weights/dinov2-base-weights.pth',\n",
       "  'freeze': True,\n",
       "  'config': {'hidden_size': 768, 'image_size': 518, 'patch_size': 14}},\n",
       " 'text_model': {'model_name': 'lstm',\n",
       "  'model_path': None,\n",
       "  'config': {'num_layers': 1, 'hidden_size': 768, 'bidirectional': False}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = get_model_config(config)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dinov2',\n",
       " './weights/dinov2-base-weights.pth',\n",
       " {'hidden_size': 768, 'image_size': 518, 'patch_size': 14},\n",
       " True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model_name, image_model_path, freeze, image_model_config = get_image_model_config(\n",
    "    model_config\n",
    ")\n",
    "image_model_name, image_model_path, image_model_config, freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/Desktop/Projects/PaperImplementations/ShowAndTell/model.py:163: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.encoder.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinov2_config = Dinov2Config(**image_model_config)\n",
    "image_encoder = load_dinov2_image_encoder(dinov2_config, freeze, image_model_path)\n",
    "dinov2_config.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs = image_encoder(batch['pixel_values'])\n",
    "# outs[0].shape, outs[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lstm', None, {'num_layers': 1, 'hidden_size': 768, 'bidirectional': False})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model_name, text_model_path, text_model_config = get_text_model_config(\n",
    "    model_config\n",
    ")\n",
    "text_model_name, text_model_path, text_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = load_lstm_text_encoder(\n",
    "    len(tokenizer), pretrained_model_path=text_model_path, **text_model_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShowAndTell Core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('showandtell', None, {})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showtell_core_model_name, showtell_core_model_path, showtell_core_config = (\n",
    "    get_show_and_tell_model_config(model_config)\n",
    ")\n",
    "showtell_core_model_name, showtell_core_model_path, showtell_core_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50259,\n",
       " 50258,\n",
       " dict_keys(['pixel_values', 'input_ids']),\n",
       " [50258,\n",
       "  1969,\n",
       "  929,\n",
       "  286,\n",
       "  41701,\n",
       "  286,\n",
       "  2057,\n",
       "  326,\n",
       "  2291,\n",
       "  44653,\n",
       "  290,\n",
       "  8509,\n",
       "  220,\n",
       "  50256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showtell_core = load_show_and_tell(\n",
    "    tokenizer,\n",
    "    image_encoder,\n",
    "    text_encoder,\n",
    "    pretrained_model_path=showtell_core_model_path,\n",
    ")\n",
    "showtell_core.vocab_size, showtell_core.tokenizer.bos_token_id, batch.keys(), batch[\n",
    "    \"input_ids\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lightning_model(tokenizer, showtell_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dry Run Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accelerator': 'gpu',\n",
       "  'max_epochs': 2,\n",
       "  'log_every_n_steps': 1,\n",
       "  'enable_progress_bar': True,\n",
       "  'overfit_batches': 1},\n",
       " 10,\n",
       " {'save_dir': './TrainingLogs/',\n",
       "  'name': 'test1',\n",
       "  'log_graph': True,\n",
       "  'prefix': ''},\n",
       " 'tensorboard')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_config, batch_size, logger_config, logger_name = get_trainer_config(config)\n",
    "trainer_config, batch_size, logger_config, logger_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([10, 14]),\n",
       " 'attention_mask': torch.Size([10, 14]),\n",
       " 'pixel_values': torch.Size([10, 3, 518, 518])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataloader\n",
    "image_text_collator = ImageTextCollator(\n",
    "    tokenizer,\n",
    "    padding=train_dataset_config.get(\"padding\", True),\n",
    "    return_tensors=train_dataset_config.get(\"return_tensors\", \"pt\"),\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, collate_fn=image_text_collator\n",
    ")\n",
    "batch = next(iter(train_dataloader))\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "logger_name = logger_config.pop\n",
    "train_logger = pl.loggers.TensorBoardLogger(**logger_config)\n",
    "trainer = pl.Trainer(logger=train_logger, **trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | showandtell_core | ShowAndTell      | 168 M  | train\n",
      "1 | criterion        | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------------\n",
      "82.0 M    Trainable params\n",
      "86.6 M    Non-trainable params\n",
      "168 M     Total params\n",
      "674.213   Total estimated model params size (MB)\n",
      "244       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/pytorch_lightning/loggers/tensorboard.py:195: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b8fb1676b14448971de3de3c80f741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([1, 14]),\n",
       " 'attention_mask': torch.Size([1, 14]),\n",
       " 'pixel_values': torch.Size([1, 3, 518, 518])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataloader\n",
    "image_text_collator = ImageTextCollator(\n",
    "    tokenizer,\n",
    "    padding=train_dataset_config.get(\"padding\", True),\n",
    "    return_tensors=train_dataset_config.get(\"return_tensors\", \"pt\"),\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=1, collate_fn=image_text_collator\n",
    ")\n",
    "batch = next(iter(val_dataloader))\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(val_dataloader))\n",
    "# pixel_values, labels = batch[\"pixel_values\"], batch[\"input_ids\"][:, 1:]\n",
    "# label_sequence = list(\n",
    "#     map(\n",
    "#         \"\".join,\n",
    "#         list(map(tokenizer.batch_decode, labels.detach().cpu().numpy().tolist())),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# logits, loss = model._step(pixel_values, labels)\n",
    "# prediction = tokenizer.batch_decode(logits.argmax(-1))\n",
    "# print(prediction, label_sequence)\n",
    "\n",
    "# from model import calculate_bleu\n",
    "\n",
    "# bleu = calculate_bleu(prediction, label_sequence)\n",
    "# print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09df9fdf750b442f9a6de5c0b332d918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        bleu_score         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8528028726577759     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     9.491268157958984     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       bleu_score        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8528028726577759    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    9.491268157958984    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 9.491268157958984, 'bleu_score': 0.8528028726577759}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, dataloaders=val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
