{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 1005, 2222, 103, 1012, 102]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"i'll [MASK].\", return_tensors='pt')\n",
    "token_ids = list(inputs['input_ids'].detach().cpu().numpy()[0])\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ll'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(2222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', \"'\", 'll', '[MASK]', '.', '[SEP]']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outs = []\n",
    "# for token_id in token_ids:\n",
    "#     print(token_id)\n",
    "#     outs.append(tokenizer.convert_ids_to_tokens())\n",
    "outs = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_scratch = BertTokenizer('./vocab.txt')\n",
    "tokenizer_scratch.convert_tokens_to_ids('mankodiya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"I'm, my dog is cute?\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]',\n",
       " {'input_ids': tensor([[  101,  1045,  1005,  1049,  1010,  2026,  3899,  2003, 10140,  1029,\n",
       "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(102), inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1049)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [101, 20228, 9496, 1011, 16834, 4442, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]},\n",
       " '[SEP]')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"pluri-potent cells\"), tokenizer.convert_ids_to_tokens(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from language import normalize_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./eng-fra.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go.', 'va !'],\n",
       " ['run!', 'cours\\u202f!'],\n",
       " ['run!', 'courez\\u202f!'],\n",
       " ['wow!', 'ca alors\\u202f!'],\n",
       " ['fire!', 'au feu !'],\n",
       " ['help!', \"a l'aide\\u202f!\"],\n",
       " ['jump.', 'saute.'],\n",
       " ['stop!', 'ca suffit\\u202f!'],\n",
       " ['stop!', 'stop\\u202f!'],\n",
       " ['stop!', 'arrete-toi !'],\n",
       " ['wait!', 'attends !'],\n",
       " ['wait!', 'attendez !'],\n",
       " ['i see.', 'je comprends.'],\n",
       " ['i try.', \"j'essaye.\"],\n",
       " ['i won!', \"j'ai gagne !\"],\n",
       " ['i won!', \"je l'ai emporte !\"],\n",
       " ['oh no!', 'oh non !'],\n",
       " ['attack!', 'attaque !'],\n",
       " ['attack!', 'attaquez !'],\n",
       " ['cheers!', 'sante !'],\n",
       " ['cheers!', 'a votre sante !'],\n",
       " ['cheers!', 'merci !'],\n",
       " ['get up.', 'leve-toi.'],\n",
       " ['got it!', \"j'ai pige !\"],\n",
       " ['got it!', 'compris !'],\n",
       " ['got it?', 'pige\\u202f?'],\n",
       " ['got it?', 'compris\\u202f?'],\n",
       " ['got it?', \"t'as capte\\u202f?\"],\n",
       " ['hop in.', 'monte.'],\n",
       " ['hop in.', 'montez.'],\n",
       " ['hug me.', 'serre-moi dans tes bras !'],\n",
       " ['hug me.', 'serrez-moi dans vos bras !'],\n",
       " ['i fell.', 'je suis tombee.'],\n",
       " ['i fell.', 'je suis tombe.'],\n",
       " ['i know.', 'je sais.'],\n",
       " ['i left.', 'je suis parti.'],\n",
       " ['i left.', 'je suis partie.'],\n",
       " ['i lost.', \"j'ai perdu.\"],\n",
       " [\"i'm 19.\", \"j'ai 19 ans.\"],\n",
       " [\"i'm ok.\", 'je vais bien.'],\n",
       " [\"i'm ok.\", 'ca va.'],\n",
       " ['listen.', 'ecoutez !'],\n",
       " ['no way!', 'impossible\\u202f!'],\n",
       " ['no way!', 'en aucun cas.'],\n",
       " ['no way!', \"c'est hors de question !\"],\n",
       " ['no way!', \"il n'en est pas question !\"],\n",
       " ['no way!', \"c'est exclu !\"],\n",
       " ['no way!', 'en aucune maniere !'],\n",
       " ['no way!', 'hors de question !'],\n",
       " ['really?', 'vraiment\\u202f?'],\n",
       " ['really?', 'vrai ?'],\n",
       " ['really?', 'ah bon ?'],\n",
       " ['thanks.', 'merci !'],\n",
       " ['we try.', 'on essaye.'],\n",
       " ['we won.', 'nous avons gagne.'],\n",
       " ['we won.', 'nous gagnames.'],\n",
       " ['we won.', \"nous l'avons emporte.\"],\n",
       " ['we won.', \"nous l'emportames.\"],\n",
       " ['ask tom.', 'demande a tom.'],\n",
       " ['awesome!', 'fantastique\\u202f!'],\n",
       " ['be calm.', 'sois calme !'],\n",
       " ['be calm.', 'soyez calme !'],\n",
       " ['be calm.', 'soyez calmes !'],\n",
       " ['be cool.', 'sois detendu !'],\n",
       " ['be fair.', 'sois juste !'],\n",
       " ['be fair.', 'soyez juste !'],\n",
       " ['be fair.', 'soyez justes !'],\n",
       " ['be fair.', 'sois equitable !'],\n",
       " ['be fair.', 'soyez equitable !'],\n",
       " ['be fair.', 'soyez equitables !'],\n",
       " ['be kind.', 'sois gentil.'],\n",
       " ['be nice.', 'sois gentil !'],\n",
       " ['be nice.', 'sois gentille !'],\n",
       " ['be nice.', 'soyez gentil !'],\n",
       " ['be nice.', 'soyez gentille !'],\n",
       " ['be nice.', 'soyez gentils !'],\n",
       " ['be nice.', 'soyez gentilles !'],\n",
       " ['beat it.', 'degage\\u202f!'],\n",
       " ['call me.', 'appelle-moi !'],\n",
       " ['call me.', 'appellez-moi !'],\n",
       " ['call us.', 'appelle-nous !'],\n",
       " ['call us.', 'appelez-nous !'],\n",
       " ['come in.', 'entrez\\u202f!'],\n",
       " ['come in.', 'entre.'],\n",
       " ['come in.', 'entre !'],\n",
       " ['come in.', 'entrez !'],\n",
       " ['come on!', 'allez\\u202f!'],\n",
       " ['come on.', 'allez !'],\n",
       " ['come on.', 'viens !'],\n",
       " ['come on.', 'venez !'],\n",
       " ['drop it!', 'laisse tomber !'],\n",
       " ['drop it!', 'laissez tomber !'],\n",
       " ['drop it!', 'laisse-le tomber !'],\n",
       " ['drop it!', 'laissez-le tomber !'],\n",
       " ['get out!', 'sortez\\u202f!'],\n",
       " ['get out!', 'sors !'],\n",
       " ['get out!', 'sortez !'],\n",
       " ['get out.', 'sors.'],\n",
       " ['get out.', 'casse-toi.'],\n",
       " ['go away!', 'degage\\u202f!'],\n",
       " ['go away!', 'pars !'],\n",
       " ['go away.', 'va te faire foutre !'],\n",
       " ['go away.', 'pars !'],\n",
       " ['go away.', 'degage !'],\n",
       " ['go away.', 'fous le camp !'],\n",
       " ['go away.', \"pars d'ici.\"],\n",
       " ['go away.', \"va t'en !\"],\n",
       " ['go away.', 'disparais !'],\n",
       " ['go away.', 'allez-vous en !'],\n",
       " ['go slow.', 'va doucement !'],\n",
       " ['go slow.', 'allez doucement !'],\n",
       " ['goodbye!', 'a la revoyure.'],\n",
       " ['hang on!', 'attends un peu !'],\n",
       " ['hang on!', 'attendez un peu !'],\n",
       " ['hang on.', 'tiens bon !'],\n",
       " ['hang on.', 'tenez bon !'],\n",
       " ['he quit.', 'il laissa tomber.'],\n",
       " ['he quit.', 'il a laisse tomber.'],\n",
       " ['he runs.', 'il court.'],\n",
       " ['help me!', 'aide-moi !'],\n",
       " ['help me.', 'aide-moi.'],\n",
       " ['help me.', 'aidez-moi.'],\n",
       " ['help us.', 'aidez-nous !'],\n",
       " ['help us.', 'aide-nous !'],\n",
       " ['hold it!', 'ne bouge plus !'],\n",
       " ['hold on.', 'ne quittez pas.'],\n",
       " ['i agree.', 'je suis du meme avis.'],\n",
       " ['i tried.', \"j'essayai.\"],\n",
       " ['i tried.', \"j'ai essaye.\"],\n",
       " ['i tried.', \"j'ai tente.\"],\n",
       " [\"i'll go.\", \"j'irai.\"],\n",
       " [\"i'm fat.\", 'je suis gras.'],\n",
       " [\"i'm fat.\", 'je suis gros.'],\n",
       " [\"i'm fit.\", 'je suis en forme.'],\n",
       " [\"i'm hit!\", 'je suis touche !'],\n",
       " [\"i'm hit!\", 'je suis touchee !'],\n",
       " [\"i'm ill.\", 'je suis malade.'],\n",
       " [\"i'm sad.\", 'je suis triste.'],\n",
       " [\"i'm shy.\", 'je suis timide.'],\n",
       " [\"i'm wet.\", 'je suis mouille.'],\n",
       " [\"i'm wet.\", 'je suis mouillee.'],\n",
       " [\"it's me!\", \"c'est bibi\\u202f!\"],\n",
       " ['join us.', 'joignez-vous.'],\n",
       " ['join us.', 'joignez-vous a nous.'],\n",
       " ['keep it.', 'garde-le !'],\n",
       " ['keep it.', 'gardez-le !'],\n",
       " ['kiss me.', 'embrasse-moi.'],\n",
       " ['kiss me.', 'embrassez-moi.'],\n",
       " ['me, too.', 'moi aussi.'],\n",
       " ['open up.', 'ouvre-moi\\u202f!'],\n",
       " ['open up.', 'ouvre.'],\n",
       " ['perfect!', 'parfait\\u202f!'],\n",
       " ['see you.', 'a plus.'],\n",
       " ['show me.', 'montre-moi !'],\n",
       " ['show me.', 'montrez-moi !'],\n",
       " ['shut up!', 'taisez-vous\\u202f!'],\n",
       " ['shut up!', 'ferme-la\\u202f!'],\n",
       " ['shut up!', 'tais-toi !'],\n",
       " ['shut up!', 'ferme-la !'],\n",
       " ['shut up!', 'la ferme !'],\n",
       " ['so long.', 'a plus tard !'],\n",
       " ['take it.', 'prends-le !'],\n",
       " ['take it.', 'prenez-le !'],\n",
       " ['tell me.', 'dis-moi !'],\n",
       " ['tell me.', 'dites-moi !'],\n",
       " ['tom won.', 'tom a gagne.'],\n",
       " ['wake up!', 'reveille-toi\\u202f!'],\n",
       " ['wake up!', 'reveille-toi !'],\n",
       " ['wake up!', 'reveillez-vous !'],\n",
       " ['wake up.', 'reveille-toi !'],\n",
       " ['wake up.', 'reveillez-vous !'],\n",
       " ['wash up.', 'lave-toi !'],\n",
       " ['wash up.', 'lavez-vous !'],\n",
       " ['we know.', 'nous savons.'],\n",
       " ['we lost.', 'nous perdimes.'],\n",
       " ['we lost.', 'nous avons perdu.'],\n",
       " ['we lost.', 'nous fumes battus.'],\n",
       " ['we lost.', 'nous fumes battues.'],\n",
       " ['we lost.', 'nous fumes defaits.'],\n",
       " ['we lost.', 'nous fumes defaites.'],\n",
       " ['we lost.', 'nous avons ete defaits.'],\n",
       " ['we lost.', 'nous avons ete defaites.'],\n",
       " ['we lost.', 'nous avons ete battus.'],\n",
       " ['we lost.', 'nous avons ete battues.'],\n",
       " ['who won?', 'qui a gagne ?'],\n",
       " ['who won?', \"qui l'a emporte ?\"],\n",
       " ['you run.', 'tu cours.'],\n",
       " ['am i fat?', 'suis-je gros ?'],\n",
       " ['am i fat?', 'suis-je grosse ?'],\n",
       " ['back off.', 'recule\\u2009!'],\n",
       " ['back off.', 'reculez.'],\n",
       " ['back off.', 'retire-toi\\u2009!'],\n",
       " ['back off.', 'retirez-vous.'],\n",
       " ['be a man.', 'sois un homme !'],\n",
       " ['be a man.', 'soyez un homme !'],\n",
       " ['be still.', 'sois calme !'],\n",
       " ['be still.', 'soyez calme !'],\n",
       " ['be still.', 'soyez calmes !'],\n",
       " ['beats me.', 'aucune idee.'],\n",
       " ['beats me.', \"j'en sais foutre rien.\"],\n",
       " ['call tom.', 'appelle tom.'],\n",
       " ['call tom.', 'appelez tom.'],\n",
       " ['cheer up!', 'courage\\u202f!'],\n",
       " ['cool off!', 'detends-toi\\u202f!'],\n",
       " ['cuff him.', 'menottez-le.'],\n",
       " ['drive on.', 'avance !'],\n",
       " ['drive on.', 'avancez !'],\n",
       " ['drive on.', 'continue a rouler !'],\n",
       " ['drive on.', 'continuez a rouler !'],\n",
       " ['get down!', 'lache-toi !'],\n",
       " ['get down.', 'descends !'],\n",
       " ['get down.', 'descendez !'],\n",
       " ['get down.', 'lache-toi !'],\n",
       " ['get down.', 'lachez-vous !'],\n",
       " ['get lost!', \"va voir ailleurs si j'y suis\\u202f!\"],\n",
       " ['get lost!', 'degage\\u202f!'],\n",
       " ['get lost!', 'va au diable !'],\n",
       " ['get real!', 'sois realiste !'],\n",
       " ['go ahead.', 'vas-y.'],\n",
       " ['go ahead.', 'poursuis !'],\n",
       " ['go ahead.', 'passe devant !'],\n",
       " ['go ahead.', 'vas-y !'],\n",
       " ['good job!', 'bien joue\\u202f!'],\n",
       " ['good job!', 'bon boulot\\u202f!'],\n",
       " ['good job!', 'beau travail\\u202f!'],\n",
       " ['grab him.', 'attrape-le.'],\n",
       " ['grab him.', 'attrapez-le.'],\n",
       " ['have fun.', 'amuse-toi bien !'],\n",
       " ['have fun.', 'amusez-vous bien !'],\n",
       " ['he tries.', 'il essaye.'],\n",
       " [\"he's wet.\", 'il est mouille.'],\n",
       " ['hi, guys.', 'salut, les mecs !'],\n",
       " ['how cute!', \"comme c'est mignon\\u202f!\"],\n",
       " ['how deep?', 'quelle profondeur\\u202f?'],\n",
       " ['how nice!', \"comme c'est chouette !\"],\n",
       " ['how nice!', \"comme c'est gentil !\"],\n",
       " ['how nice!', \"c'est du joli !\"],\n",
       " ['how nice!', \"comme c'est agreable !\"],\n",
       " ['humor me.', 'fais-moi rire.'],\n",
       " ['hurry up.', 'depeche-toi.'],\n",
       " ['hurry up.', 'grouille\\u202f!'],\n",
       " ['hurry up.', 'pressez-vous !'],\n",
       " ['hurry up.', 'fica !'],\n",
       " ['i am fat.', 'je suis gras.'],\n",
       " ['i did ok.', \"je m'en suis bien sorti.\"],\n",
       " ['i did ok.', \"je m'en suis bien sortie.\"],\n",
       " ['i did it.', \"je l'ai fait.\"],\n",
       " ['i did it.', \"c'est moi qui l'ai fait.\"],\n",
       " ['i forgot.', \"j'ai oublie.\"],\n",
       " ['i get it.', \"j'ai compris.\"],\n",
       " ['i got it.', \"j'ai compris.\"],\n",
       " ['i got it.', \"j'ai capte.\"],\n",
       " ['i phoned.', 'je telephonai.'],\n",
       " ['i phoned.', \"j'ai telephone.\"],\n",
       " ['i refuse.', 'je refuse.'],\n",
       " ['i refuse.', 'je le refuse.'],\n",
       " ['i saw it.', \"je l'ai vu.\"],\n",
       " ['i saw it.', 'je l’ai vu.'],\n",
       " ['i stayed.', 'je suis reste.'],\n",
       " ['i stayed.', 'je suis restee.'],\n",
       " ['i use it.', \"je l'utilise.\"],\n",
       " ['i use it.', \"j'en fais usage.\"],\n",
       " ['i use it.', \"je m'en sers.\"],\n",
       " [\"i'll pay.\", 'je paierai.'],\n",
       " [\"i'll try.\", \"j'essaierai.\"],\n",
       " [\"i'm back.\", 'je suis revenu.'],\n",
       " [\"i'm back.\", 'me revoila.'],\n",
       " [\"i'm bald.\", 'je suis chauve.'],\n",
       " [\"i'm busy.\", 'je suis occupe.'],\n",
       " [\"i'm busy.\", 'je suis occupee.'],\n",
       " [\"i'm calm.\", 'je suis calme.'],\n",
       " [\"i'm cold.\", \"j'ai froid.\"],\n",
       " [\"i'm done.\", \"j'en ai fini.\"],\n",
       " [\"i'm fine.\", 'tout va bien.'],\n",
       " [\"i'm fine.\", 'je vais bien.'],\n",
       " [\"i'm fine.\", 'ca va.'],\n",
       " [\"i'm free!\", 'je suis libre !'],\n",
       " [\"i'm free.\", 'je suis libre.'],\n",
       " [\"i'm free.\", 'je suis disponible.'],\n",
       " [\"i'm full.\", 'je suis repu\\u202f!'],\n",
       " [\"i'm full.\", 'je suis rassasie\\u202f!'],\n",
       " [\"i'm glad.\", 'je suis content.'],\n",
       " [\"i'm home.\", 'je suis chez moi.'],\n",
       " [\"i'm late.\", 'je suis en retard.'],\n",
       " [\"i'm lazy.\", 'je suis paresseux.'],\n",
       " [\"i'm lazy.\", 'je suis faineant.'],\n",
       " [\"i'm lazy.\", 'je suis paresseuse.'],\n",
       " [\"i'm lazy.\", 'je suis faineante.'],\n",
       " [\"i'm okay.\", 'je vais bien.'],\n",
       " [\"i'm okay.\", 'je me porte bien.'],\n",
       " [\"i'm safe.\", 'je suis en securite.'],\n",
       " [\"i'm sick.\", 'je suis malade.'],\n",
       " [\"i'm sure.\", \"j'en suis certain.\"],\n",
       " [\"i'm sure.\", 'je suis certain.'],\n",
       " [\"i'm sure.\", \"j'en suis sur.\"],\n",
       " [\"i'm sure.\", \"j'en suis sure.\"],\n",
       " [\"i'm tall.\", 'je suis grande.'],\n",
       " [\"i'm thin.\", 'je suis mince.'],\n",
       " [\"i'm tidy.\", 'je suis ordonne.'],\n",
       " [\"i'm tidy.\", 'je suis ordonnee.'],\n",
       " [\"i'm ugly.\", 'je suis laid.'],\n",
       " [\"i'm ugly.\", 'je suis laide.'],\n",
       " [\"i'm weak.\", 'je suis faible.'],\n",
       " [\"i'm well.\", 'je vais bien.'],\n",
       " [\"i'm well.\", 'je me porte bien.'],\n",
       " [\"i've won.\", \"j'ai gagne.\"],\n",
       " [\"i've won.\", \"je l'ai emporte.\"],\n",
       " ['it works.', 'elle marche.'],\n",
       " ['it works.', 'ca fonctionne.'],\n",
       " [\"it's his.\", \"c'est le sien.\"],\n",
       " [\"it's his.\", \"c'est la sienne.\"],\n",
       " [\"it's new.\", \"c'est nouveau.\"],\n",
       " [\"it's new.\", \"c'est neuf.\"],\n",
       " [\"it's odd.\", \"c'est bizarre.\"],\n",
       " [\"it's sad.\", 'c’est triste.'],\n",
       " ['keep out!', \"defense d'entrer.\"],\n",
       " ['keep out.', \"n'entrez pas.\"],\n",
       " ['leave it.', 'laisse tomber !'],\n",
       " ['leave it.', 'laissez tomber !'],\n",
       " ['leave me.', 'laissez-moi !'],\n",
       " ['leave us.', 'laisse-nous !'],\n",
       " ['leave us.', 'laissez-nous !'],\n",
       " [\"let's go!\", 'allons-y !'],\n",
       " [\"let's go!\", 'allons !'],\n",
       " [\"let's go.\", 'allons-y !'],\n",
       " ['look out!', 'attention !'],\n",
       " ['marry me.', 'epouse-moi !'],\n",
       " ['marry me.', 'epousez-moi !'],\n",
       " ['may i go?', 'puis-je partir ?'],\n",
       " ['may i go?', 'puis-je y aller ?'],\n",
       " ['may i go?', \"puis-je m'y rendre ?\"],\n",
       " ['she came.', 'elle est venue.'],\n",
       " ['she died.', 'elle est morte.'],\n",
       " ['she runs.', 'elle court.'],\n",
       " ['sit down!', 'assieds-toi !'],\n",
       " ['sit down!', 'asseyez-vous !'],\n",
       " ['sit here.', 'assieds-toi ici.'],\n",
       " ['sit here.', 'asseyez-vous ici.'],\n",
       " ['speak up!', 'parle plus fort\\u202f!'],\n",
       " ['speak up!', 'parlez plus fort\\u202f!'],\n",
       " ['stop tom.', 'arrete tom.'],\n",
       " ['stop tom.', 'stoppez tom.'],\n",
       " ['terrific!', 'genial\\u202f!'],\n",
       " ['terrific!', 'excellent\\u202f!'],\n",
       " ['terrific!', 'formidable !'],\n",
       " ['they won.', 'ils gagnerent.'],\n",
       " ['they won.', 'elles gagnerent.'],\n",
       " ['they won.', 'ils ont gagne.'],\n",
       " ['they won.', 'elles ont gagne.'],\n",
       " ['tom came.', 'tom est venu.'],\n",
       " ['tom died.', 'tom est mort.'],\n",
       " ['tom left.', 'tom est parti.'],\n",
       " ['tom left.', 'tom partit.'],\n",
       " ['tom lost.', 'tom a perdu.'],\n",
       " ['too late.', 'trop tard.'],\n",
       " ['trust me.', 'faites-moi confiance.'],\n",
       " ['trust me.', 'fais-moi confiance.'],\n",
       " ['try hard.', 'fais un effort.'],\n",
       " ['try some.', 'essaies-en !'],\n",
       " ['try some.', 'essayez-en !'],\n",
       " ['try this.', 'essaie ceci !'],\n",
       " ['try this.', 'essayez ceci !'],\n",
       " ['use this.', 'utilise ceci.'],\n",
       " ['use this.', 'utilisez ceci.'],\n",
       " ['use this.', 'emploie ceci !'],\n",
       " ['use this.', 'employez ceci !'],\n",
       " ['warn tom.', 'avertis tom.'],\n",
       " ['warn tom.', 'previens tom.'],\n",
       " ['watch me.', 'regarde-moi !'],\n",
       " ['watch me.', 'regardez-moi !'],\n",
       " ['watch us.', 'regardez-nous !'],\n",
       " ['watch us.', 'regarde-nous !'],\n",
       " ['we agree.', \"nous sommes d'accord.\"],\n",
       " [\"we'll go.\", 'nous irons.'],\n",
       " ['what for?', 'pour quoi faire\\u202f?'],\n",
       " ['what for?', 'a quoi bon ?'],\n",
       " ['what fun!', \"qu'est-ce qu'on s'est marres !\"],\n",
       " ['what fun!', \"qu'est-ce qu'on s'est marrees !\"],\n",
       " ['who died?', 'qui est mort ?'],\n",
       " [\"who's he?\", 'qui est-il\\u202f?'],\n",
       " ['write me.', 'ecris-moi !'],\n",
       " ['write me.', 'ecrivez-moi !'],\n",
       " ['after you.', 'apres vous.'],\n",
       " ['aim. fire!', 'en joue ! feu !'],\n",
       " ['am i late?', 'suis-je en retard ?'],\n",
       " ['answer me.', 'repondez-moi.'],\n",
       " ['be seated.', 'assieds-toi !'],\n",
       " ['be seated.', 'asseyez-vous !'],\n",
       " ['birds fly.', 'les oiseaux volent.'],\n",
       " ['bless you.', 'a tes souhaits\\u202f!'],\n",
       " ['call home!', 'appelle a la maison !'],\n",
       " ['calm down!', 'calmez-vous !'],\n",
       " ['calm down.', 'calme-toi.'],\n",
       " ['can we go?', 'pouvons-nous partir ?'],\n",
       " ['can we go?', 'pouvons-nous nous en aller ?'],\n",
       " ['can we go?', 'pouvons-nous y aller ?'],\n",
       " ['catch him.', 'rattrape-le.'],\n",
       " ['come back.', 'reviens !'],\n",
       " ['come back.', 'revenez !'],\n",
       " ['come here.', 'viens ici.'],\n",
       " ['come here.', 'venez la.'],\n",
       " ['come over!', 'viens\\u202f!'],\n",
       " ['come over!', 'venez\\u202f!'],\n",
       " ['come over.', 'venez ici !'],\n",
       " ['come over.', 'viens chez nous !'],\n",
       " ['come over.', 'venez chez nous !'],\n",
       " ['come over.', 'viens chez moi !'],\n",
       " ['come over.', 'venez chez moi !'],\n",
       " ['come soon.', 'viens bientot !'],\n",
       " ['come soon.', 'venez bientot !'],\n",
       " ['cool down.', 'calmez-vous !'],\n",
       " ['did i win?', 'ai-je gagne ?'],\n",
       " ['did i win?', \"l'ai-je emporte ?\"],\n",
       " ['did i win?', 'est-ce moi qui ai gagne ?'],\n",
       " ['dogs bark.', 'des chiens aboient.'],\n",
       " ['dogs bark.', 'les chiens aboient.'],\n",
       " [\"don't ask.\", 'ne demande pas !'],\n",
       " [\"don't cry.\", 'ne pleure pas !'],\n",
       " [\"don't die.\", 'ne meurs pas !'],\n",
       " [\"don't die.\", 'ne mourez pas !'],\n",
       " [\"don't lie.\", 'ne mens pas.'],\n",
       " [\"don't run.\", 'ne courez pas.'],\n",
       " [\"don't run.\", 'ne cours pas.'],\n",
       " ['excuse me.', 'excuse-moi.'],\n",
       " ['fantastic!', 'fantastique\\u202f!'],\n",
       " ['feel this.', 'sens ca !'],\n",
       " ['feel this.', 'sentez ca !'],\n",
       " ['feel this.', 'touche ca !'],\n",
       " ['feel this.', 'touchez ca !'],\n",
       " ['follow me.', 'suis-moi.'],\n",
       " ['follow us.', 'suis-nous !'],\n",
       " ['follow us.', 'suivez-nous !'],\n",
       " ['forget it!', 'oublie !'],\n",
       " ['forget it!', 'oublie-le !'],\n",
       " ['forget it!', 'oubliez !'],\n",
       " ['forget it!', 'oubliez-le !'],\n",
       " ['forget it.', 'laisse tomber.'],\n",
       " ['forget it.', 'oublie.'],\n",
       " ['get a job.', 'trouve un emploi !'],\n",
       " ['get a job.', 'trouve un boulot !'],\n",
       " ['get a job.', 'trouvez un emploi !'],\n",
       " ['get a job.', 'trouvez un boulot !'],\n",
       " ['get ready.', 'prepare-toi.'],\n",
       " ['get ready.', 'preparez-vous.'],\n",
       " ['go get it.', 'va le chercher !'],\n",
       " ['go get it.', 'allez le chercher !'],\n",
       " ['go inside.', 'entrez\\u202f!'],\n",
       " ['go to bed.', 'va au lit !'],\n",
       " ['go to bed.', 'allez au lit !'],\n",
       " ['good luck.', 'bonne chance\\u202f!'],\n",
       " ['good luck.', 'bonne chance.'],\n",
       " ['grab that.', 'attrape ca !'],\n",
       " ['grab that.', 'attrapez ca !'],\n",
       " ['grab that.', 'saisis-toi de ca !'],\n",
       " ['grab that.', 'saisissez-vous de ca !'],\n",
       " ['grab this.', 'attrape ca !'],\n",
       " ['grab this.', 'attrapez ca !'],\n",
       " ['hands off.', 'pas touche\\u202f!'],\n",
       " ['he is ill.', 'il est malade.'],\n",
       " ['he is old.', 'il est vieux.'],\n",
       " [\"he's a dj.\", 'il est dj.'],\n",
       " [\"he's good.\", 'il est bon.'],\n",
       " [\"he's lazy.\", 'il est paresseux.'],\n",
       " [\"he's rich.\", 'il est riche.'],\n",
       " ['here i am.', 'me voici.'],\n",
       " [\"here's $5.\", 'voila cinq dollars.'],\n",
       " ['hold fire.', 'halte au feu !'],\n",
       " ['hold fire.', 'cessez le feu !'],\n",
       " ['hold this.', 'tiens ca !'],\n",
       " ['hold this.', 'tenez ca !'],\n",
       " ['hold this.', 'tenez ceci !'],\n",
       " ['hold this.', 'tiens ceci !'],\n",
       " ['how awful!', \"c'est affreux\\u202f!\"],\n",
       " [\"how's tom?\", 'comment tom va-t-il ?'],\n",
       " [\"how's tom?\", 'comment va tom ?'],\n",
       " ['i am busy.', 'je suis occupe.'],\n",
       " ['i am calm.', 'je suis calme.'],\n",
       " ['i am cold.', \"j'ai froid.\"],\n",
       " ['i am good.', 'je suis bon.'],\n",
       " ['i am here.', 'je suis ici.'],\n",
       " ['i am lazy.', 'je suis paresseux.'],\n",
       " ['i am lazy.', 'je suis faineant.'],\n",
       " ['i am lazy.', 'je suis paresseuse.'],\n",
       " ['i am lazy.', 'je suis faineante.'],\n",
       " ['i am okay.', 'je vais bien.'],\n",
       " ['i am sick.', 'je suis malade.'],\n",
       " ['i am sure.', 'je suis sur.'],\n",
       " ['i am sure.', 'je suis certain.'],\n",
       " ['i am weak.', 'je suis faible.'],\n",
       " ['i beg you.', 'je vous en prie.'],\n",
       " ['i beg you.', 'je vous en conjure.'],\n",
       " ['i beg you.', 'je vous en supplie.'],\n",
       " ['i beg you.', 'je te prie.'],\n",
       " ['i can run.', 'je sais courir.'],\n",
       " ['i can ski.', 'je sais skier.'],\n",
       " ['i cringed.', \"j'eus un mouvement de recul.\"],\n",
       " ['i cringed.', \"j'ai eu un mouvement de recul.\"],\n",
       " ['i cringed.', 'je suis rentre en moi-meme.'],\n",
       " ['i give up.', \"j'abandonne.\"],\n",
       " ['i got hot.', 'je me suis mis a avoir chaud.'],\n",
       " ['i got hot.', 'je me suis mise a avoir chaud.'],\n",
       " ['i had fun.', 'je me suis amuse.'],\n",
       " ['i had fun.', 'je me suis amusee.'],\n",
       " ['i had fun.', 'je me suis marre.'],\n",
       " ['i had fun.', 'je me suis marree.'],\n",
       " ['i hate it.', 'je deteste ca.'],\n",
       " ['i hope so.', \"j'espere bien.\"],\n",
       " ['i knew it.', 'je le savais.'],\n",
       " ['i like it.', \"j'aime ca.\"],\n",
       " ['i lost it.', 'je l’ai perdu.'],\n",
       " ['i love it!', \"j'adore ca !\"],\n",
       " ['i love it.', \"j'adore ca !\"],\n",
       " ['i mean it!', 'je suis serieux\\u202f!'],\n",
       " ['i mean it.', 'je suis serieux.'],\n",
       " ['i must go.', 'je dois y aller.'],\n",
       " ['i must go.', \"il faut que j'y aille.\"],\n",
       " ['i must go.', 'il me faut y aller.'],\n",
       " ['i must go.', 'il me faut partir.'],\n",
       " ['i must go.', \"il me faut m'en aller.\"],\n",
       " ['i must go.', 'je dois partir.'],\n",
       " ['i must go.', \"je dois m'en aller.\"],\n",
       " ['i must go.', \"il faut que je m'en aille.\"],\n",
       " ['i need it.', \"j'en ai besoin.\"],\n",
       " ['i need it.', 'il me le faut.'],\n",
       " ['i noticed.', \"j'ai remarque.\"],\n",
       " ['i promise.', 'je le promets.'],\n",
       " ['i said no.', \"j'ai dit non.\"],\n",
       " ['i said so.', \"je l'ai dit.\"],\n",
       " ['i saw him.', \"je l'ai vu.\"],\n",
       " ['i saw him.', 'je l’ai vu.'],\n",
       " ['i saw him.', 'je le vis.'],\n",
       " ['i saw one.', \"j'en ai vu une.\"],\n",
       " ['i saw one.', \"j'en ai vu un.\"],\n",
       " ['i saw you.', 'je vous vis.'],\n",
       " ['i saw you.', 'je te vis.'],\n",
       " ['i saw you.', \"je t'ai vue.\"],\n",
       " ['i saw you.', \"je t'ai vu.\"],\n",
       " ['i saw you.', 'je vous ai vues.'],\n",
       " ['i saw you.', 'je vous ai vus.'],\n",
       " ['i saw you.', 'je vous ai vue.'],\n",
       " ['i saw you.', 'je vous ai vu.'],\n",
       " ['i see tom.', 'je vois tom.'],\n",
       " ['i tripped.', \"j'ai trebuche.\"],\n",
       " ['i tripped.', \"j'ai plane.\"],\n",
       " ['i want it.', 'je le veux.'],\n",
       " ['i was new.', \"j'etais nouveau.\"],\n",
       " ['i was new.', \"j'etais nouvelle.\"],\n",
       " ['i will go.', \"j'irai.\"],\n",
       " ['i woke up.', 'je me suis reveille.'],\n",
       " ['i woke up.', 'je me suis eveille.'],\n",
       " [\"i'd agree.\", \"je serais d'accord.\"],\n",
       " [\"i'd leave.\", 'je partirais.'],\n",
       " [\"i'll call.\", \"j'appellerai.\"],\n",
       " [\"i'll cook.\", 'je cuisinerai.'],\n",
       " [\"i'll help.\", \"j'aiderai.\"],\n",
       " [\"i'll live.\", 'je vivrai.'],\n",
       " [\"i'll obey.\", \"j'obeirai.\"],\n",
       " [\"i'll pack.\", 'je ferai mon sac.'],\n",
       " [\"i'll pack.\", 'je ferai ma valise.'],\n",
       " [\"i'll pack.\", 'je plierai mes gaules.'],\n",
       " [\"i'll pass.\", 'je passerai.'],\n",
       " [\"i'll quit.\", \"j'abandonnerai.\"],\n",
       " [\"i'll sing.\", 'je chanterai.'],\n",
       " [\"i'll swim.\", 'je nagerai.'],\n",
       " [\"i'll wait.\", \"j'attendrai.\"],\n",
       " [\"i'll walk.\", 'je marcherai.'],\n",
       " [\"i'll work.\", 'je vais travailler.'],\n",
       " [\"i'll work.\", 'je travaillerai.'],\n",
       " [\"i'm a cop.\", 'je suis flic.'],\n",
       " [\"i'm a man.\", 'je suis un homme.'],\n",
       " [\"i'm alone.\", 'je suis seule.'],\n",
       " [\"i'm alone.\", 'je suis seul.'],\n",
       " [\"i'm armed.\", 'je suis arme.'],\n",
       " [\"i'm armed.\", 'je suis armee.'],\n",
       " [\"i'm awake.\", 'je suis reveille.'],\n",
       " [\"i'm blind.\", 'je suis aveugle.'],\n",
       " [\"i'm broke.\", 'je suis fauche.'],\n",
       " [\"i'm crazy.\", 'je suis fou.'],\n",
       " [\"i'm crazy.\", 'je suis folle.'],\n",
       " [\"i'm cured.\", 'je suis gueri.'],\n",
       " [\"i'm cured.\", 'je suis guerie.'],\n",
       " [\"i'm drunk.\", 'je suis saoul.'],\n",
       " [\"i'm drunk.\", 'je suis soul.'],\n",
       " [\"i'm drunk.\", 'je suis ivre.'],\n",
       " [\"i'm dying.\", 'je me meurs.'],\n",
       " [\"i'm early.\", 'je suis en avance.'],\n",
       " [\"i'm first.\", 'je suis en premier.'],\n",
       " [\"i'm fussy.\", 'je suis difficile.'],\n",
       " [\"i'm fussy.\", 'je suis tatillon.'],\n",
       " [\"i'm fussy.\", 'je suis tatillonne.'],\n",
       " [\"i'm going.\", 'je pars maintenant.'],\n",
       " [\"i'm going.\", 'je me tire.'],\n",
       " [\"i'm going.\", 'j’y vais.'],\n",
       " [\"i'm going.\", 'je pars.'],\n",
       " [\"i'm loyal.\", 'je suis loyal.'],\n",
       " [\"i'm loyal.\", 'je suis loyale.'],\n",
       " [\"i'm lucky.\", 'je suis veinard.'],\n",
       " [\"i'm lucky.\", 'je suis veinarde.'],\n",
       " [\"i'm lucky.\", \"j'ai du pot.\"],\n",
       " [\"i'm lucky.\", 'je suis chanceux.'],\n",
       " [\"i'm lucky.\", 'je suis chanceuse.'],\n",
       " [\"i'm lying.\", 'je suis en train de mentir.'],\n",
       " [\"i'm quiet.\", 'je suis tranquille.'],\n",
       " [\"i'm ready!\", 'je suis prete !'],\n",
       " [\"i'm ready!\", 'je suis pret !'],\n",
       " [\"i'm ready.\", 'je suis pret.'],\n",
       " [\"i'm right.\", \"j'ai raison.\"],\n",
       " [\"i'm sober.\", 'je suis sobre.'],\n",
       " [\"i'm sorry.\", 'excuse-moi.'],\n",
       " [\"i'm sorry.\", 'desole.'],\n",
       " [\"i'm sorry.\", 'desole !'],\n",
       " [\"i'm sorry.\", 'je suis desole.'],\n",
       " [\"i'm sorry.\", 'je suis desolee.'],\n",
       " [\"i'm stuck.\", 'je suis coincee.'],\n",
       " [\"i'm timid.\", 'je suis timide.'],\n",
       " [\"i'm tired.\", 'je suis fatigue !'],\n",
       " [\"i'm tough.\", 'je suis dur.'],\n",
       " [\"i'm tough.\", 'je suis dure.'],\n",
       " [\"i'm tough.\", 'je suis dur a cuire.'],\n",
       " [\"i'm tough.\", 'je suis dure a cuire.'],\n",
       " [\"i'm yours.\", 'je suis a toi.'],\n",
       " [\"i'm yours.\", 'je suis a vous.'],\n",
       " [\"i've lost.\", \"j'ai perdu.\"],\n",
       " ['is tom ok?', 'est-ce que tom va bien ?'],\n",
       " ['is tom ok?', 'tom va-t-il bien ?'],\n",
       " ['is it bad?', \"c'est grave\\u202f?\"],\n",
       " ['is it far?', 'est-ce eloigne ?'],\n",
       " ['is it far?', 'est-ce loin ?'],\n",
       " ['is it you?', 'est-ce toi ?'],\n",
       " ['is it you?', 'est-ce vous ?'],\n",
       " ['is it you?', \"est-ce que c'est vous ?\"],\n",
       " ['it failed.', 'ca a echoue.'],\n",
       " ['it snowed.', 'il a neige.'],\n",
       " ['it stinks.', 'ca sent mauvais.'],\n",
       " ['it stinks.', 'ca pue.'],\n",
       " ['it worked.', 'ca a fonctionne.'],\n",
       " ['it worked.', 'ca a marche.'],\n",
       " [\"it's 3:30.\", 'il est trois heures et demie.'],\n",
       " [\"it's cold.\", 'il fait froid.'],\n",
       " [\"it's dark.\", \"c'est sombre.\"],\n",
       " [\"it's dead.\", 'elle est morte.'],\n",
       " [\"it's dead.\", \"c'est mort.\"],\n",
       " [\"it's dead.\", 'il est mort.'],\n",
       " [\"it's done.\", \"c'est fait.\"],\n",
       " [\"it's food.\", \"c'est de la nourriture.\"],\n",
       " [\"it's free.\", \"c'est gratuit.\"],\n",
       " [\"it's hers.\", \"c'est le sien.\"],\n",
       " [\"it's hers.\", \"c'est la sienne.\"],\n",
       " [\"it's late.\", 'il est tard.'],\n",
       " [\"it's lost.\", \"c'est perdu.\"],\n",
       " [\"it's open.\", \"c'est ouvert.\"],\n",
       " [\"it's sand.\", \"c'est du sable.\"],\n",
       " [\"it's true!\", \"c'est vrai\\u202f!\"],\n",
       " [\"it's work.\", \"c'est du boulot.\"],\n",
       " ['let it be.', 'ainsi soit-il.'],\n",
       " ['let it be.', 'laisse faire.'],\n",
       " ['let me go!', 'laisse-moi partir\\u202f!'],\n",
       " ['let me go!', 'laissez-moi partir\\u202f!'],\n",
       " ['let me go!', 'lache-moi\\u202f!'],\n",
       " ['let me go!', \"laisse-moi m'en aller !\"],\n",
       " ['let me go!', \"laissez-moi m'en aller !\"],\n",
       " ['let me go!', 'laissez-moi y aller !'],\n",
       " ['let me go!', 'laisse-moi y aller !'],\n",
       " ['let me go.', 'laisse-moi partir\\u202f!'],\n",
       " ['let me go.', 'laissez-moi partir\\u202f!'],\n",
       " ['let me go.', \"laisse-moi m'en aller !\"],\n",
       " ['let me go.', \"laissez-moi m'en aller !\"],\n",
       " ['let me in.', 'laissez-moi rentrer.'],\n",
       " ['let me in.', 'laissez-moi entrer.'],\n",
       " [\"let's ask.\", 'demandons.'],\n",
       " [\"let's see.\", 'voyons voir !'],\n",
       " ['lie still.', 'reste allonge, immobile !'],\n",
       " ['lie still.', 'reste allongee, immobile !'],\n",
       " ['lie still.', 'restez allonge, immobile !'],\n",
       " ['lie still.', 'restez allongee, immobile !'],\n",
       " ['lie still.', 'restez allonges, immobiles !'],\n",
       " ['lie still.', 'restez allongees, immobiles !'],\n",
       " ['loosen up.', 'echauffe-toi !'],\n",
       " ['loosen up.', 'echauffez-vous !'],\n",
       " ['loosen up.', 'detends-toi !'],\n",
       " ['loosen up.', 'laisse-toi aller !'],\n",
       " ['loosen up.', 'laissez-vous aller !'],\n",
       " ['nice shot!', 'joli coup !'],\n",
       " ['of course!', 'pour sur.'],\n",
       " ['of course!', 'mais ouais !'],\n",
       " ['of course.', 'bien sur.'],\n",
       " ['of course.', 'pour sur.'],\n",
       " ['oh please!', 'je vous en prie !'],\n",
       " ['oh please!', \"je t'en prie !\"],\n",
       " ['pardon me?', 'pardon\\u202f?'],\n",
       " ['pardon me?', 'je vous demande pardon\\u202f?'],\n",
       " ['pardon me?', 'plait-il\\u202f?'],\n",
       " ['read this.', 'lis ceci.'],\n",
       " ['say hello.', 'dis bonjour.'],\n",
       " ['see above.', 'voyez ci-dessus.'],\n",
       " ['seriously?', 'vraiment\\u202f?'],\n",
       " ['seriously?', 'est-ce serieux\\u202f?'],\n",
       " ['seriously?', 'serieusement ?'],\n",
       " ['she cried.', 'elle pleurait.'],\n",
       " ['she cried.', 'elle pleura.'],\n",
       " ['she tried.', 'elle a essaye.'],\n",
       " ['she walks.', 'elle marche.'],\n",
       " [\"she's hot.\", 'elle est chaude.'],\n",
       " [\"she's hot.\", 'elle est tres attirante.'],\n",
       " ['sign here.', 'signe ici.'],\n",
       " ['sign here.', 'signez ici.'],\n",
       " ['slow down.', 'ralentis !'],\n",
       " ['slow down.', 'ralentissez !'],\n",
       " ['stay back.', 'reste en arriere !'],\n",
       " ['stay back.', 'restez en arriere !'],\n",
       " ['stay calm.', 'restez calme.'],\n",
       " ['stay calm.', 'reste calme.'],\n",
       " ['stay calm.', 'garde ton calme.'],\n",
       " ['stay calm.', 'garde ton sang-froid.'],\n",
       " ['stay calm.', 'reste tranquille.'],\n",
       " ['stay down.', 'reste baisse.'],\n",
       " ['stay down.', 'restez baisse.'],\n",
       " ['stay thin.', 'reste mince !'],\n",
       " ['stop that!', 'arretez !'],\n",
       " ['stop that.', 'arretez ca !'],\n",
       " ['stop that.', 'arrete ca !'],\n",
       " ['take care.', 'prends soin de toi.'],\n",
       " ['take care.', 'prenez soin de vous.'],\n",
       " ['take mine.', 'prends le mien.'],\n",
       " ['take mine.', 'prends la mienne.'],\n",
       " ['take mine.', 'prenez le mien.'],\n",
       " ['take mine.', 'prenez la mienne.'],\n",
       " ['take mine.', 'prends les miens.'],\n",
       " ['take mine.', 'prends les miennes.'],\n",
       " ['take mine.', 'prenez les miens.'],\n",
       " ['take mine.', 'prenez les miennes.'],\n",
       " ['take this.', 'prends ca.'],\n",
       " ['take this.', 'prenez ca.'],\n",
       " ['thank you.', 'merci !'],\n",
       " [\"that's it.\", \"c'est ca.\"],\n",
       " ['they fell.', 'ils sont tombes.'],\n",
       " ['they fell.', 'elles sont tombees.'],\n",
       " ['they left.', 'ils sont partis.'],\n",
       " ['they left.', 'elles sont parties.'],\n",
       " ['they lied.', 'ils ont menti.'],\n",
       " ['they lied.', 'elles ont menti.'],\n",
       " ['they lost.', 'ils ont perdu.'],\n",
       " ['they lost.', 'elles ont perdu.'],\n",
       " ['they swam.', 'ils nageaient.'],\n",
       " ['they swam.', 'elles nageaient.'],\n",
       " ['they swam.', 'ils nagerent.'],\n",
       " ['they swam.', 'elles nagerent.'],\n",
       " ['tom knits.', 'tom tricote.'],\n",
       " ['tom knows.', 'tom sait.'],\n",
       " ['tom spoke.', 'tom a parle.'],\n",
       " [\"tom's fat.\", 'tom est gros.'],\n",
       " ['try again.', 'essaie encore.'],\n",
       " ['try again.', 'essayez de nouveau.'],\n",
       " ['try again.', 'essaie de nouveau.'],\n",
       " ['try it on.', 'essaie-le\\u2009!'],\n",
       " ['turn left.', 'tourne a gauche.'],\n",
       " ['wait here.', 'attends ici.'],\n",
       " ['wait here.', 'attends la.'],\n",
       " ['wait here.', 'attendez ici.'],\n",
       " ['wait here.', 'attendez la.'],\n",
       " ['watch out!', 'attention !'],\n",
       " ['watch out!', 'faites attention\\u202f!'],\n",
       " ['watch out!', 'fais attention\\u202f!'],\n",
       " ['we agreed.', \"nous sommes tombes d'accord.\"],\n",
       " ['we did it!', 'nous avons reussi\\u202f!'],\n",
       " ['we did it.', 'nous avons reussi\\u202f!'],\n",
       " ['we did it.', \"nous l'avons fait.\"],\n",
       " ['we forgot.', 'nous avons oublie.'],\n",
       " ['we saw it.', \"nous l'avons vu.\"],\n",
       " ['we saw it.', \"nous l'avons vue.\"],\n",
       " ['we talked.', 'nous discutames.'],\n",
       " ['we talked.', 'nous avons discute.'],\n",
       " ['we talked.', 'nous nous sommes entretenus.'],\n",
       " ['we talked.', 'nous nous sommes entretenues.'],\n",
       " ['we talked.', 'nous nous entretinmes.'],\n",
       " ['we waited.', 'nous attendimes.'],\n",
       " ['we waited.', 'nous avons attendu.'],\n",
       " [\"we'll try.\", 'nous essayerons.'],\n",
       " [\"we'll try.\", 'nous tenterons.'],\n",
       " [\"we'll win.\", \"nous l'emporterons.\"],\n",
       " [\"we'll win.\", 'nous gagnerons.'],\n",
       " [\"we're hot.\", 'nous avons chaud.'],\n",
       " [\"we're sad.\", 'nous sommes tristes.'],\n",
       " [\"we're shy.\", 'nous sommes timides.'],\n",
       " ['well done!', 'bien vu\\u202f!'],\n",
       " ['well done!', 'bien cuit\\u202f!'],\n",
       " ['well done!', 'a la bonne heure\\u202f!'],\n",
       " ['well done!', 'pas mal !'],\n",
       " ['what else?', 'quoi d’autre\\u202f?'],\n",
       " ['what else?', \"quoi d'autre ?\"],\n",
       " [\"what's up?\", 'quoi de beau\\u202f?'],\n",
       " ['who cares?', \"qui s'en preoccupe\\u202f?\"],\n",
       " ['who cares?', \"qui s'en soucie\\u202f?\"],\n",
       " ['who cares?', 'a qui ceci importe-t-il\\u202f?'],\n",
       " ['who is he?', 'qui est-ce\\u202f?'],\n",
       " ['who is he?', 'qui est-il\\u202f?'],\n",
       " ['who is it?', 'qui est-ce\\u202f?'],\n",
       " ['who is it?', 'qui est-il\\u202f?'],\n",
       " ['who knows?', 'qui sait\\u202f?'],\n",
       " ['who spoke?', 'qui a parle ?'],\n",
       " [\"who'll go?\", 'qui ira ?'],\n",
       " [\"who's ill?\", 'qui est malade ?'],\n",
       " ['wonderful!', 'magnifique\\u202f!'],\n",
       " ['write tom.', 'ecrivez a tom.'],\n",
       " ['you drive.', 'tu conduis.'],\n",
       " ['you drive.', 'toi, conduis !'],\n",
       " ['you drive.', 'vous conduisez.'],\n",
       " ['you idiot!', \"espece d'imbecile\\u202f!\"],\n",
       " ['you idiot!', \"espece d'idiot\\u202f!\"],\n",
       " ['you start.', 'tu commences.'],\n",
       " ['you start.', 'vous commencez.'],\n",
       " ['you tried.', 'tu as essaye.'],\n",
       " ['you tried.', 'vous avez essaye.'],\n",
       " ['all aboard!', 'tout le monde a bord!'],\n",
       " ['am i clear?', 'suis-je clair ?'],\n",
       " ['am i clear?', 'suis-je claire ?'],\n",
       " ['am i dying?', 'suis-je en train de mourir ?'],\n",
       " ['am i dying?', 'suis-je en train de trepasser ?'],\n",
       " ['am i early?', 'suis-je en avance ?'],\n",
       " ['am i fired?', 'vous me mettez a la porte ?'],\n",
       " ['am i first?', 'suis-je le premier ?'],\n",
       " ['am i first?', 'suis-je la premiere ?'],\n",
       " ['am i hired?', 'suis-je engage ?'],\n",
       " ['am i hired?', 'suis-je engagee ?'],\n",
       " ['am i right?', 'ai-je raison ?'],\n",
       " ['am i right?', 'suis-je dans le vrai ?'],\n",
       " ['am i wrong?', 'me trompe-je\\u202f?'],\n",
       " ['am i wrong?', 'ai-je tort ?'],\n",
       " ['am i wrong?', 'est-ce que je me trompe ?'],\n",
       " ['are you ok?', 'est-ce que tu vas bien\\u202f?'],\n",
       " ['are you up?', 'es-tu leve ?'],\n",
       " ['are you up?', 'es-tu levee ?'],\n",
       " ['are you up?', 'etes-vous leve ?'],\n",
       " ['are you up?', 'etes-vous levee ?'],\n",
       " ['are you up?', 'etes-vous leves ?'],\n",
       " ['are you up?', 'etes-vous levees ?'],\n",
       " ['are you up?', 'es-tu debout ?'],\n",
       " ['are you up?', 'etes-vous debout ?'],\n",
       " ['ask anyone.', \"demandez a n'importe qui.\"],\n",
       " ['ask anyone.', \"demande a n'importe qui.\"],\n",
       " ['ask anyone.', 'demande a quiconque !'],\n",
       " ['ask around.', 'demande alentour !'],\n",
       " ['ask around.', 'demandez alentour !'],\n",
       " ['be careful.', 'sois prudent !'],\n",
       " ['be careful.', 'sois prudente !'],\n",
       " ['be careful.', 'soyez prudent !'],\n",
       " ['be careful.', 'soyez prudente !'],\n",
       " ['be careful.', 'soyez prudents !'],\n",
       " ['be careful.', 'soyez prudentes !'],\n",
       " ['be content.', 'sois satisfait !'],\n",
       " ['be content.', 'sois satisfaite !'],\n",
       " ['be content.', 'soyez satisfait !'],\n",
       " ['be content.', 'soyez satisfaite !'],\n",
       " ['be content.', 'soyez satisfaits !'],\n",
       " ['be content.', 'soyez satisfaites !'],\n",
       " ['be serious.', 'soyez serieux !'],\n",
       " ['be serious.', 'soyez serieuse !'],\n",
       " ['be serious.', 'soyez serieuses !'],\n",
       " ['be serious.', 'sois serieux !'],\n",
       " ['be serious.', 'sois serieuse !'],\n",
       " ['birds sing.', 'les oiseaux chantent.'],\n",
       " ['bottoms up!', 'sante !'],\n",
       " ['can i come?', 'puis-je venir ?'],\n",
       " ['can i help?', 'puis-je aider\\u202f?'],\n",
       " ['can i stay?', 'puis-je rester ?'],\n",
       " ['check this.', 'verifie ca.'],\n",
       " ['check this.', 'regarde ca.'],\n",
       " ['choose one.', 'choisis-en un.'],\n",
       " ['choose one.', 'choisis-en une.'],\n",
       " ['come along.', 'joignez-vous a nous.'],\n",
       " ['come on in!', 'entre donc\\u202f!'],\n",
       " ['come on in.', 'entre.'],\n",
       " ['come quick!', 'viens vite\\u202f!'],\n",
       " ['come quick!', 'jouis vite\\u202f!'],\n",
       " ['come to me.', 'venez a moi.'],\n",
       " ['come to us.', 'venez a nous.'],\n",
       " ['cut it out!', 'arrete !'],\n",
       " ['cut it out!', 'arretez !'],\n",
       " ['cut it out.', 'assez\\u202f!'],\n",
       " ['did we win?', 'avons-nous gagne ?'],\n",
       " ['do i stink?', 'est-ce que je pue ?'],\n",
       " ['do come in!', 'je vous en prie, entrez\\u202f!'],\n",
       " ['do men cry?', 'les hommes pleurent-ils ?'],\n",
       " [\"don't fret.\", 'te tracasse pas.'],\n",
       " [\"don't fret.\", 'vous tracassez pas.'],\n",
       " [\"don't fret.\", 'ne te tracasse pas.'],\n",
       " [\"don't fret.\", 'ne vous tracassez pas.'],\n",
       " [\"don't move!\", 'ne bouge pas !'],\n",
       " [\"don't move!\", 'ne bougez pas !'],\n",
       " [\"don't move.\", 'ne bougez pas.'],\n",
       " [\"don't move.\", 'ne bouge pas.'],\n",
       " [\"don't rush.\", 'ne te precipite pas.'],\n",
       " [\"don't rush.\", 'ne vous precipitez pas.'],\n",
       " [\"don't talk!\", 'ne parle pas !'],\n",
       " [\"don't talk!\", 'ne parlez pas !'],\n",
       " [\"don't wait.\", \"n'attends pas !\"],\n",
       " [\"don't wait.\", \"n'attendez pas !\"],\n",
       " ['duty calls.', \"le devoir m'appelle.\"],\n",
       " ['fill it up.', 'le plein.'],\n",
       " ['find a job.', 'trouve un emploi !'],\n",
       " ['find a job.', 'trouve un boulot !'],\n",
       " ['follow him.', 'suis-le\\u202f!'],\n",
       " ['follow him.', 'suivez-le\\u202f!'],\n",
       " ['forget him.', 'oublie-le.'],\n",
       " ['forget him.', 'oubliez-le.'],\n",
       " ['forgive me.', 'pardonnez-moi.'],\n",
       " ['get a life!', 'achete-toi une vie !'],\n",
       " ['get to bed.', 'va au lit !'],\n",
       " ['get to bed.', 'au lit !'],\n",
       " ['get to bed.', 'allez au lit !'],\n",
       " ['give it up.', 'laisse tomber.'],\n",
       " ['give it up.', 'abandonne\\u202f!'],\n",
       " ['go warm up.', \"va t'echauffer !\"],\n",
       " ['go warm up.', 'allez vous echauffer !'],\n",
       " ['he gave in.', 'il a cede.'],\n",
       " ['he gave in.', 'il ceda.'],\n",
       " ['he hung up.', 'il a raccroche.'],\n",
       " ['he hung up.', 'il raccrocha.'],\n",
       " ['he is a dj.', 'il est dj.'],\n",
       " ['he is busy.', 'il a a faire.'],\n",
       " ['he is here!', 'il est ici\\u202f!'],\n",
       " ['he is kind.', 'il est gentil.'],\n",
       " ['he is late.', 'il est en retard.'],\n",
       " ['he is lazy.', 'il est faineant.'],\n",
       " ['he is lazy.', 'il est paresseux.'],\n",
       " ['he is poor.', 'il est pauvre.'],\n",
       " ['he is sick.', 'il est malade.'],\n",
       " ['he made it.', 'il a reussi.'],\n",
       " [\"he's swiss.\", 'il est suisse.'],\n",
       " [\"he's swiss.\", 'il est helvete.'],\n",
       " [\"he's broke.\", 'il est ruine.'],\n",
       " [\"he's broke.\", 'il est fauche.'],\n",
       " [\"he's drunk.\", 'il est ivre.'],\n",
       " [\"he's drunk.\", 'il est soul.'],\n",
       " [\"he's smart.\", 'il est intelligent.'],\n",
       " ['here he is!', 'il est ici\\u202f!'],\n",
       " ['here it is.', 'le voila.'],\n",
       " ['here it is.', 'tenez.'],\n",
       " ['here it is.', 'le voici.'],\n",
       " ['here we go.', \"c'est parti !\"],\n",
       " ['here we go.', 'on est partis !'],\n",
       " ['hold still.', 'tiens-toi tranquille !'],\n",
       " ['hold still.', 'tenez-vous tranquille !'],\n",
       " ['how lovely!', \"comme c'est charmant !\"],\n",
       " [\"how's work?\", 'comment va le travail ?'],\n",
       " ['hurry home.', \"depeche-toi d'aller chez toi !\"],\n",
       " ['hurry home.', \"depechez-vous d'aller chez vous !\"],\n",
       " ['i am a man.', 'je suis un homme.'],\n",
       " ['i am human.', 'je suis humain.'],\n",
       " ['i am ready.', 'je suis pret.'],\n",
       " ['i am right.', \"j'ai raison.\"],\n",
       " ['i am sorry.', 'je suis desole.'],\n",
       " ['i am sorry.', 'je suis desolee.'],\n",
       " ['i am tired.', 'je suis fatigue !'],\n",
       " ['i am tired.', 'je suis creve.'],\n",
       " ['i broke it.', \"je l'ai casse.\"],\n",
       " ['i broke it.', \"je l'ai cassee.\"],\n",
       " ['i built it.', \"je l'ai construit.\"],\n",
       " ['i built it.', \"je l'ai construite.\"],\n",
       " ['i can come.', 'je peux venir.'],\n",
       " ['i can cook.', 'je sais cuisiner.'],\n",
       " ['i can jump.', 'je peux sauter.'],\n",
       " ['i can read.', 'je sais lire.'],\n",
       " ['i can read.', 'je peux lire.'],\n",
       " ['i can sing.', 'je sais chanter.'],\n",
       " ['i can stay.', 'je peux rester.'],\n",
       " ['i can swim.', 'je sais nager.'],\n",
       " ['i can wait.', 'je peux attendre.'],\n",
       " ['i can walk.', 'je peux marcher.'],\n",
       " [\"i can't go.\", 'je ne peux pas y aller.'],\n",
       " [\"i can't go.\", \"je ne peux pas m'y rendre.\"],\n",
       " [\"i can't go.\", 'je ne peux pas partir.'],\n",
       " ['i did that.', \"j'ai fait cela.\"],\n",
       " ['i did that.', \"c'est moi qui l'ai fait.\"],\n",
       " ['i disagree.', \"je ne suis pas d'accord.\"],\n",
       " ['i do worry.', 'je me fais vraiment du souci.'],\n",
       " ['i doubt it.', \"j'en doute.\"],\n",
       " ['i eat here.', 'je mange ici.'],\n",
       " ['i envy him.', \"je l'envie.\"],\n",
       " ['i envy you.', 'je vous envie.'],\n",
       " ['i envy you.', \"je t'envie.\"],\n",
       " ['i feel bad.', 'je me sens mal.'],\n",
       " ['i feel old.', 'je me sens vieux.'],\n",
       " ['i feel old.', 'je me sens vieille.'],\n",
       " ['i felt bad.', 'je me suis senti mal.'],\n",
       " ['i felt ill.', 'je me suis senti mal.'],\n",
       " ['i felt ill.', 'je me sentais malade.'],\n",
       " ['i felt ill.', 'je me sentis malade.'],\n",
       " ['i felt sad.', \"j'ai ressenti de la tristesse.\"],\n",
       " ['i fixed it.', \"je l'ai repare.\"],\n",
       " ['i fixed it.', \"je l'ai reparee.\"],\n",
       " ['i fixed it.', 'je la reparai.'],\n",
       " ['i fixed it.', 'je le reparai.'],\n",
       " ['i found it.', \"je l'ai trouve.\"],\n",
       " ['i found it.', 'je le trouvai.'],\n",
       " ['i got busy.', 'je suis devenu occupe.'],\n",
       " ['i got busy.', 'je suis devenu occupee.'],\n",
       " ['i got lost.', 'je me suis perdu.'],\n",
       " ['i got sick.', 'je suis devenu malade.'],\n",
       " ['i got sick.', 'je suis devenue malade.'],\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('run!', 'run!')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1][0], pairs[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs = pd.DataFrame(data=pairs, columns=['english', 'french'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sent = '\\n'.join(list(df_pairs['english'].drop_duplicates().values))\n",
    "french_sent = '\\n'.join(list(df_pairs['french'].drop_duplicates().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./english.txt', 'w') as f:\n",
    "    f.writelines(english_sent)\n",
    "    \n",
    "with open('./french.txt', 'w') as f:\n",
    "    f.writelines(french_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'care'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"take [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# # mask labels of non-[MASK] tokens\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 30522]),\n",
       " tensor([6]),\n",
       " {'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " tensor([3000]),\n",
       " 'paris')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, mask_token_index, inputs, predicted_token_id, tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 8, 1, 4, 8, 2, 2, 7, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random = torch.randint(low=1, high=10, size=(10, ))\n",
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "        0.1500])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full(random.shape, .15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def mask_tokens(input_ids, tokenizer, mlm_probability=0.15):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling (MLM).\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): Tensor of token ids\n",
    "        tokenizer (PreTrainedTokenizer): Hugging Face tokenizer\n",
    "        mlm_probability (float): Probability of masking a token (15% default)\n",
    "    Returns:\n",
    "        Tuple of (input_ids, labels) where labels is the original token ids\n",
    "        for the masked positions and input_ids is the input tensor with masks.\n",
    "    \"\"\"\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Masking logic: we randomly select 15% of tokens to mask\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    \n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace 80% of masked tokens with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    input_ids[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # Replace 10% of masked tokens with random tokens\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.1)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    input_ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # 10% of masked tokens are left unchanged\n",
    "    return input_ids, labels\n",
    "\n",
    "# Example usage with a sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# # Apply the masking\n",
    "# masked_input_ids, labels = mask_tokens(input_ids, tokenizer)\n",
    "\n",
    "# # Forward pass through the model\n",
    "# outputs = model(input_ids=masked_input_ids, labels=labels)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits\n",
    "\n",
    "# print(f\"Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1996,  4248,  2829,  4419,   103,  2058,  1996, 13971,  3899,\n",
       "           102]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
       "            102]]),\n",
       " tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "          0.1500, 0.1500]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = input_ids.clone()\n",
    "probability_matrix = torch.full(labels.shape, 0.15)\n",
    "labels, probability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_mask = [\n",
    "    tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "    for val in labels.tolist()\n",
    "]\n",
    "special_tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False,  True,\n",
       "         False]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 3899, -100]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[~masked_indices] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "          False]]),\n",
       " tensor([[False, False, False, False, False, False, False, False, False,  True,\n",
       "          False]]),\n",
       " tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "          False]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "input_ids[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "indices_replaced, masked_indices, indices_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11394,  5532, 12200, 26389,  8699, 15066, 15568, 10737, 11721, 15001,\n",
       "           3006]]),\n",
       " 30522,\n",
       " tensor([], dtype=torch.int64))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_random = torch.bernoulli(torch.full(labels.shape, 0.1)).bool() & masked_indices & ~indices_replaced\n",
    "random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "input_ids[indices_random] = random_words[indices_random]\n",
    "random_words, len(tokenizer), input_ids[indices_random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3696, 2182, 2067]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['sign', 'here', 'back'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
